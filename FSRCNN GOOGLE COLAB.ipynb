{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled10.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"DLMSotwMAJRQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["%matplotlib inline\n","import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from skimage import io, color\n","\n","\n","SF = 2   #scaling factor, i.e. how much do you want to scale your image (here, twice of original image)\n","learning_rate = 0.01\n","ip_height = 160\n","ip_width = 240"],"execution_count":0,"outputs":[]},{"metadata":{"id":"R_aT6o2uAPsp","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["temp1 = io.imread_collection('home/wazir/Desktop/baboonLR.png')\n","temp2 = io.imread_collection('home/wazir/Desktop/baboonHR.png')\n","\n","xx = np.array([images for i,images in enumerate(temp1)],dtype =np.float32).reshape(-1,160,240,1)\n","yy = np.array([images for i,images in enumerate(temp2)],dtype= np.float32).reshape(-1,1,320*480)\n","\n","\n","X = tf.placeholder(tf.float32, shape=(None, 160, 240, 1),name=\"X\" )        # input\n","Y = tf.placeholder(tf.float32,shape=(None, 1, 320*480),name=\"Y\" )      # actual output\n","\n","#print(xx.shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"O376jR2mAtjL","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def conv2d(x, W, b, strides=1, act_fn = 1):\n","    # Conv2D wrapper, with bias and relu activation\n","    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n","    x = tf.nn.bias_add(x, b)\n","    if act_fn ==1:\n","        return tf.nn.relu(x)\n","    else:\n","        return x\n","\n","def dil_conv2d(x, W, b, rate=2):\n","    # Dilation Conv2D wrapper, with bias and but no activation\n","    x = tf.nn.atrous_conv2d(x, W , rate, padding='SAME')\n","    x = tf.nn.bias_add(x, b)\n","    return x"],"execution_count":0,"outputs":[]},{"metadata":{"id":"S3OYgmZSAyLP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Store layer's weight & bias   ------   weights here are equivalent to 'filters' in tensorflow\n","\n","def CNN(X):\n","    weights = {\n","        # 5x5 conv, 1 input, 16 outputs                             FEATURE REP.\n","        'wc1': tf.Variable(tf.random_normal([5, 5, 1, 16]),name='wc1',dtype=tf.float32),\n","    \n","        # 5x5 conv, 56 inputs, 8 outputs                           SHRINKING\n","        'wc2': tf.Variable(tf.random_normal([5, 5, 16, 8]),'wc2',dtype=tf.float32),\n","    \n","        # 5x5 conv, 8 inputs, 8 outputs                           NON-LINEAR MAPPING\n","        'wc3': tf.Variable(tf.random_normal([5, 5, 8, 8]), 'wc3',dtype=tf.float32),\n","        # 3X3 dilation conv, 12 inputs, 12 outputs                  NON-LINEAR MAPPING\n","        'wd1': tf.Variable(tf.random_normal([3, 3, 8,8]),'wd1',dtype=tf.float32),\n","        # 5x5 conv, 12 inputs, 12 outputs                           NON-LINEAR MAPPING\n","        'wc4': tf.Variable(tf.random_normal([5, 5, 8,8]),'wc4',dtype=tf.float32),\n","        # 3X3 dilation conv, 12 inputs, 12 outputs                  NON-LINEAR MAPPING\n","        'wd2': tf.Variable(tf.random_normal([3, 3, 8,8]),'wd2',dtype=tf.float32),\n","        # 5x5 conv, 12 inputs, 12 outputs                           NON-LINEAR MAPPING\n","        'wc5': tf.Variable(tf.random_normal([5, 5, 8,8]),'wc5',dtype=tf.float32),\n","\n","        # 5x5 conv, 12 inputs, 56 outputs                           EXPANSION\n","        'wc6': tf.Variable(tf.random_normal([5, 5, 8, 16]),'wc6',dtype=tf.float32),\n","\n","        #the last convolution, 16 inputs, 2 outputs\n","        'wc7': tf.Variable(tf.random_normal([5, 5, 16, 1]),'wc7',dtype=tf.float32),\n","        'wc8': tf.Variable(tf.random_normal([4, 4, 1, 16]),'wc8',dtype=tf.float32)\n","    }\n","\n","    biases = {\n","        'bc1': tf.Variable(tf.random_normal([16]),dtype=tf.float32),    #FEATURE REPRESENTATION\n","\n","        'bc2': tf.Variable(tf.random_normal([8]),dtype=tf.float32),     #SHRINKING\n","\n","        'bc3': tf.Variable(tf.random_normal([8]),dtype=tf.float32),     #NON-LINEAR MAPPING\n","        'bd1': tf.Variable(tf.random_normal([8]),dtype=tf.float32),     #NON-LINEAR MAPPING\n","        'bc4': tf.Variable(tf.random_normal([8]),dtype=tf.float32),     #NON-LINEAR MAPPING\n","        'bd2': tf.Variable(tf.random_normal([8]),dtype=tf.float32),     #NON-LINEAR MAPPING\n","        'bc5': tf.Variable(tf.random_normal([8]),dtype=tf.float32),     #NON-LINEAR MAPPING\n","\n","        'bc6': tf.Variable(tf.random_normal([16]),dtype=tf.float32),    #EXPANSION\n","\n","        'bc7': tf.Variable(tf.random_normal([1]),dtype=tf.float32)     #DECONV LAYER (not reqd.)\n","    }\n","\n","\n","    X = tf.cast(tf.reshape(X, shape=[-1, 160, 240, 1]),tf.float32)\n","\n","    c1 = conv2d(X, weights['wc1'], biases['bc1'])    #FEATURE REP\n","\n","    c2 = conv2d(c1, weights['wc2'], biases['bc2'])          #SHRINKING\n","\n","    c3 = conv2d(c2, weights['wc3'], biases['bc3'])          #NON-LINEAR MAPPING\n","    d1 = dil_conv2d(c3, weights['wd1'], biases['bd1'])\n","    c4 = conv2d(d1, weights['wc4'], biases['bc4'])\n","    d2 = dil_conv2d(c4, weights['wd2'], biases['bd2'])\n","    c5 = conv2d(d2, weights['wc5'], biases['bc5'])\n","\n","    c6 = conv2d(c5, weights['wc6'], biases['bc6'], act_fn =0)     #EXPANSION (LOCAL QUEUE JUMPING)\n","    c6 = tf.add(c1,c6)\n","    c6 = tf.nn.relu(c6)\n","    \n","    op_image =tf.contrib.layers.conv2d_transpose(c6, 1, 4, 2, padding=\"same\")\n","    op_image = tf.reshape(op_image, ( -1,1, 320*480),\"op_image\")\n","    \n","    return op_image"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UWtubA2cA2IH","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def train_cnn(X):\n","    prediction = CNN(X)\n","    #mse\n","    loss = tf.reduce_mean(tf.pow(tf.subtract(prediction, Y), 2.0))\n","    #optimizer\n","    train = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n","\n","    with tf.Session() as sess:\n","        sess.run(tf.global_variables_initializer())\n","        \n","        for epoch in range(5):\n","            epoch_loss = 0\n","            \n","            c = sess.run(loss, feed_dict={X: xx, Y: yy})\n","            b = sess.run(train, feed_dict={X: xx, Y: yy})\n","            print('Epoch', epoch, 'completed out of 10  loss:', c)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0_qsl4xJA5Sa","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":106},"outputId":"ad629fc3-7694-48f8-9bcb-eba4ab76b5ed","executionInfo":{"status":"ok","timestamp":1531038655682,"user_tz":-420,"elapsed":7416,"user":{"displayName":"niaz muhammad","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114165712822211084731"}}},"cell_type":"code","source":["train_cnn(X)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Epoch 0 completed out of 10  loss: nan\n","Epoch 1 completed out of 10  loss: nan\n","Epoch 2 completed out of 10  loss: nan\n","Epoch 3 completed out of 10  loss: nan\n","Epoch 4 completed out of 10  loss: nan\n"],"name":"stdout"}]},{"metadata":{"id":"uQvKM8KFA8c3","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}